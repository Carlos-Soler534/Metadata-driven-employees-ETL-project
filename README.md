# ETL Data Pipeline Project (Medallion Architecture)

This project implements a modular and scalable ETL pipeline using PySpark and follows the **Medallion Architecture** (Bronze, Silver, Gold). It is designed to run on **Azure Databricks** and reads configuration from external metadata files to orchestrate the flow.

---

## ğŸ—‚ï¸ Project Structure

project_root/
â”‚
â”œâ”€â”€ main.py # Main orchestrator script for the ETL pipeline
â”œâ”€â”€ config/
â”‚ â””â”€â”€ metadata.json # Metadata configuration used to drive the ETL process
â”‚
â”œâ”€â”€ etl/
â”‚ â”œâ”€â”€ ingestion/
â”‚ â”‚ â””â”€â”€ DataIngestor.py # Contains logic to ingest raw data into Bronze layer
â”‚ â”‚
â”‚ â””â”€â”€ transformation/
â”‚ â””â”€â”€ DataValidatorAndTransformer.py # Handles validations and transformations to Silver
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ logger_config.py # Centralized logger configuration
â”‚ â””â”€â”€ etl_pipeline.log # (Optional) log file created during execution

## âš™ï¸ Features

- ğŸ” **Metadata-driven pipeline**: dynamically loads sources, sinks, and transformations from `metadata.json`.
- ğŸ¥‰ **Bronze Layer**: stores raw ingested data.
- ğŸ¥ˆ **Silver Layer**: stores cleaned and validated data.
- ğŸ¥‡ **Gold Layer**: optionally generates aggregated views.
- âœ… **Validation**: Invalid rows are logged and saved separately.
- ğŸ“„ **Logging**: Console and file logging via a reusable utility.
- ğŸ”§ **Job orchestration ready**: Compatible with Databricks Jobs with task dependencies.

---

## ğŸš€ How to Run

### 1. Upload to Databricks Workspace (or clone locally)

Clone this repository and open in a Databricks notebook or attach to a cluster if you're using a script-based workflow.

### 2. Prepare Your Metadata

Edit `config/metadata.json` to configure your sources, sinks, and transformation logic.

### 3. Run the Pipeline

Run the `main.py` script to execute the ETL flow:

```bash
python main.py
